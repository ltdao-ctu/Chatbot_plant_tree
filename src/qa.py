import sys, io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import numpy as np
import faiss
import json
import requests
from sentence_transformers import SentenceTransformer, CrossEncoder

# Gi·∫£ s·ª≠ b·∫°n ƒë√£ c√≥:
# - embedder: SentenceTransformer
# - index: FAISS index
# - docs: danh s√°ch metadata (docs.json ƒë√£ load s·∫µn)


MODEL_NAME = "intfloat/multilingual-e5-small"
INDEX_FILE = "faiss.index"
META_FILE = "docs.json"
RERANK_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"

# --------------------------------------------
# üß† Load model + d·ªØ li·ªáu
embedder = SentenceTransformer(MODEL_NAME)
index = faiss.read_index(INDEX_FILE)
reranker = CrossEncoder(RERANK_MODEL)
with open(META_FILE, "r", encoding="utf-8") as f:
    docs = json.load(f)




# === Function: retrieve context ===
def retrieve(query, top_k=30, rerank_top_n=9):
    # 1. T√≠nh embedding cho query
    qv = embedder.encode([query], normalize_embeddings=True).astype("float32")

    # 2. L·∫•y top_k t·ª´ FAISS
    D, I = index.search(qv, top_k)
    candidates = [docs[idx] for idx in I[0] if idx != -1]

    # 3. Rerank b·∫±ng CrossEncoder
    pairs = [(query, c["text"]) for c in candidates]
    scores = reranker.predict(pairs)

    # 4. Gh√©p ƒëi·ªÉm v√† s·∫Øp x·∫øp
    reranked = sorted(
        zip(candidates, scores),
        key=lambda x: x[1],
        reverse=True
    )

    # 5. Tr·∫£ v·ªÅ top_n k·∫øt qu·∫£ cu·ªëi c√πng
    results = [
        {
            "rank": i+1,
            "source": r[0]["source"],
            "rep_type": r[0]["rep_type"],
            "score": float(r[1]),
            "text": r[0]["text"][:500] + "..."  # c·∫Øt ng·∫Øn khi in
        }
        for i, r in enumerate(reranked[:rerank_top_n])
    ]

    return results



# === Function: build prompt ===
# def make_prompt(query, retrieved):
#     parts = []
#     for i, r in enumerate(retrieved, 1):
#         parts.append(f"[{i}] (source: {r['source']}) {r['text'][:800]}")
#     context = "\n\n".join(parts)
#     prompt = f"B·∫°n l√† tr·ª£ l√Ω tr·ªìng c√¢y. D·ª±a tr√™n th√¥ng tin sau:\n{context}\n\nC√¢u h·ªèi: {query}\nTr·∫£ l·ªùi ng·∫Øn g·ªçn v√† n√™u ngu·ªìn."
#     return prompt

def make_prompt(query: str, retrieved: list, role: str = "tr·ª£ l√Ω tr·ªìng c√¢y") -> str:
    """
    T·∫°o prompt ho√†n ch·ªânh cho LLM d·ª±a tr√™n c√°c ƒëo·∫°n vƒÉn ƒë∆∞·ª£c truy xu·∫•t.
    
    Args:
        query (str): C√¢u h·ªèi ng∆∞·ªùi d√πng.
        retrieved (list): Danh s√°ch t√†i li·ªáu (ƒë√£ qua retrieve + rerank).
        role (str): Vai tr√≤ c·ªßa h·ªá th·ªëng tr·ª£ l√Ω.
    
    Returns:
        str: Prompt ho√†n ch·ªânh s·∫µn s√†ng g·ª≠i v√†o LLM.
    """

    if not retrieved:
        return f"Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan cho c√¢u h·ªèi: {query}"

    # üß© Gh√©p ng·ªØ c·∫£nh t·ª´ c√°c ƒëo·∫°n truy xu·∫•t
    parts = []
    for i, r in enumerate(retrieved, 1):
        # Gi·ªõi h·∫°n text ƒë·ªÉ tr√°nh prompt qu√° d√†i
        snippet = (r.get("text") or "").strip().replace("\n", " ")
        snippet = snippet[:800] + ("..." if len(snippet) > 800 else "")

        parts.append(f"[{i}] (Ngu·ªìn: {r.get('source', 'kh√¥ng r√µ')})\n{snippet}")

    context = "\n\n".join(parts)

    # üß† C·∫•u tr√∫c prompt chu·∫©n RAG
    prompt = (
        f"B·∫°n l√† {role}, c√≥ nhi·ªám v·ª• tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë√£ cho.\n\n"
        f"=== NG·ªÆ C·∫¢NH ===\n{context}\n\n"
        f"=== C√ÇU H·ªéI ===\n{query}\n\n"
        f"=== Y√äU C·∫¶U ===\n"
        f"- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c.\n"
        f"- N·∫øu c√≥ th·ªÉ, h√£y n√™u r√µ ngu·ªìn (s·ªë th·ª© t·ª± trong ngo·∫∑c vu√¥ng).\n"
    )

    return prompt

# === Function: call Ollama API ===
# def call_ollama(prompt, model="gemma:2b"):
#     url = "http://localhost:11434/api/generate"
#     payload = {
#         "model": model,   # b·∫°n c√≥ th·ªÉ ƒë·ªïi sang "mistral", "gemma:2b", v.v.
#         "prompt": prompt,
#         "stream": False
#     }
#     resp = requests.post(url, json=payload)
#     if resp.status_code == 200:
#         return resp.json()["response"]
#     else:
#         return f"L·ªói Ollama API: {resp.text}"

def call_ollama(prompt: str, model: str = "gemma:2b", temperature: float = 0.7, max_tokens: int = 1024) -> str:
    """
    G·ªçi API c·ªßa Ollama ƒë·ªÉ sinh ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh ng√¥n ng·ªØ c·ª•c b·ªô.

    Args:
        prompt (str): Chu·ªói prompt ƒë·∫ßu v√†o (ƒë√£ bao g·ªìm context v√† c√¢u h·ªèi).
        model (str): T√™n m√¥ h√¨nh Ollama (vd: "gemma:2b", "mistral", "llama3", ...).
        temperature (float): M·ª©c ƒë·ªô s√°ng t·∫°o c·ªßa m√¥ h√¨nh (0.0 - 1.0).
        max_tokens (int): Gi·ªõi h·∫°n s·ªë token sinh ra.

    Returns:
        str: Ph·∫£n h·ªìi vƒÉn b·∫£n t·ª´ m√¥ h√¨nh ho·∫∑c th√¥ng b√°o l·ªói.
    """
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": temperature,
            "num_predict": max_tokens
        }
    }

    try:
        resp = requests.post(url, json=payload, timeout=120)
        resp.raise_for_status()  # n√©m l·ªói n·∫øu status != 200
        data = resp.json()
        return data.get("response", "(Kh√¥ng c√≥ ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh)")
    except requests.exceptions.ConnectionError:
        return "‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn Ollama. H√£y ƒë·∫£m b·∫£o d·ªãch v·ª• ƒëang ch·∫°y (ollama serve)."
    except requests.exceptions.Timeout:
        return "‚ö†Ô∏è Y√™u c·∫ßu t·ªõi Ollama b·ªã qu√° th·ªùi gian ch·ªù."
    except requests.exceptions.JSONDecodeError:
        return f"‚ö†Ô∏è Ph·∫£n h·ªìi kh√¥ng h·ª£p l·ªá: {resp.text[:200]}"
    except Exception as e:
        return f"‚ö†Ô∏è L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}"
    

# === Main answer function ===
# def answer(query, top_k=3, model="gemma:2b"):
#     retrieved = retrieve(query, top_k=top_k)
#     if not retrieved:
#         return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong c∆° s·ªü d·ªØ li·ªáu."

#     # In ra top-k ƒë·ªÉ theo d√µi
#     print("\n=== Retrieved context ===")
#     for i, r in enumerate(retrieved, 1):
#         print(f"[{i}] (source: {r['source']}) {r['text'][:200]}...")
#     print("=========================\n")

#     prompt = make_prompt(query, retrieved)
#     print(f"[DEBUG] Prompt length: {len(prompt)} chars\n")

#     return call_ollama(prompt, model=model)



def answer(query: str, top_k: int = 5, model: str = "gemma:2b", debug: bool = True) -> str:
    """
    Truy v·∫•n h·ªá th·ªëng RAG: retrieve ‚Üí t·∫°o prompt ‚Üí g·ªçi Ollama ‚Üí tr·∫£ l·ªùi.

    Args:
        query (str): C√¢u h·ªèi ng∆∞·ªùi d√πng.
        top_k (int): S·ªë ƒëo·∫°n vƒÉn l·∫•y t·ª´ FAISS (tr∆∞·ªõc khi rerank).
        model (str): M√¥ h√¨nh Ollama c·∫ßn g·ªçi (vd: "gemma:2b", "mistral").
        debug (bool): N·∫øu True, in log truy xu·∫•t v√† prompt.

    Returns:
        str: C√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c sinh ra b·ªüi m√¥ h√¨nh.
    """
    try:
        # 1Ô∏è‚É£ Retrieve (l·∫•y d·ªØ li·ªáu li√™n quan)
        retrieved = retrieve(query, top_k=top_k)
        if not retrieved:
            return "‚ùå Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu."

        if debug:
            print("\n=== üìö Retrieved context ===")
            for i, r in enumerate(retrieved, 1):
                snippet = r["text"].replace("\n", " ")[:200]
                print(f"[{i}] ({r.get('rep_type', '-')}) {r['source']}: {snippet}...")
            print("============================\n")

        # 2Ô∏è‚É£ T·∫°o prompt cho LLM
        prompt = make_prompt(query, retrieved)
        if debug:
            print(f"[DEBUG] Prompt length: {len(prompt)} chars\n")

        # 3Ô∏è‚É£ G·ªçi Ollama LLM ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi
        answer = call_ollama(prompt, model=model)
        return answer

    except Exception as e:
        return f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {e}"
