# # # ingest.py (phi√™n b·∫£n multi-representation RAG - kh√¥ng chunk t·ª± ƒë·ªông)
# # # ---------------------------------------------------------------
# # # M·ª•c ti√™u:
# # #   - Duy·ªát qua t·∫•t c·∫£ file .docx (ho·∫∑c vƒÉn b·∫£n) trong th∆∞ m·ª•c ch·ªâ ƒë·ªãnh
# # #   - M·ªói file ƒë∆∞·ª£c embedding nhi·ªÅu t·∫ßng bi·ªÉu di·ªÖn (multi-representation)
# # #       + raw: to√†n b·ªô n·ªôi dung
# # #       + summary: t√≥m t·∫Øt n·ªôi dung (ng·∫Øn h∆°n)
# # #       + keywords: tr√≠ch xu·∫•t c√°c ch·ªß t·ª´ ch√≠nh
# # #   - L∆∞u v√†o FAISS + metadata JSON ƒë·ªÉ ph·ª•c v·ª• RAG ƒëa t·∫ßng
# # import sys, io
# # sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')


# # import os, json, uuid
# # import numpy as np
# # import faiss
# # from sentence_transformers import SentenceTransformer
# # from extractors import auto_extract  # t·ª± ƒë·ªông tr√≠ch xu·∫•t n·ªôi dung file docx, pdf, txt, v.v.
# # from utils import extract_summary, extract_keywords  # gi·∫£ ƒë·ªãnh b·∫°n c√≥ 2 h√†m t√≥m t·∫Øt & tr√≠ch ch·ªß t·ª´

# # # ---------------------------------------------------------------
# # # üîß C·∫•u h√¨nh
# # DATA_DIR = "data_output"             # th∆∞ m·ª•c ch·ª©a file c·∫ßn ingest
# # INDEX_FILE = "faiss.index"
# # META_FILE = "docs.json"

# # # model embedding nh·∫π, h·ªó tr·ª£ ƒëa ng√¥n ng·ªØ (t·ªët cho ti·∫øng Vi·ªát)
# # MODEL_NAME = "intfloat/multilingual-e5-small"
# # embedder = SentenceTransformer(MODEL_NAME)
# # dimension = embedder.get_sentence_embedding_dimension()

# # # ---------------------------------------------------------------
# # # üß† T·∫°o ho·∫∑c load FAISS index
# # if os.path.exists(INDEX_FILE):
# #     index = faiss.read_index(INDEX_FILE)
# #     with open(META_FILE, "r", encoding="utf-8") as f:
# #         docs = json.load(f)
# # else:
# #     # D√πng cosine similarity (inner product + normalize vector)
# #     index = faiss.IndexFlatIP(dimension)
# #     docs = []

# # # ---------------------------------------------------------------
# # # üìÑ H√†m x·ª≠ l√Ω 1 file ƒë∆°n l·∫ª
# # def ingest_file(path):
# #     text = auto_extract(path)
# #     if not text.strip():
# #         print(f"[!] B·ªè qua file r·ªóng: {path}")
# #         return

# #     # T·∫°o c√°c bi·ªÉu di·ªÖn kh√°c nhau cho c√πng 1 t√†i li·ªáu
# #     representations = {
# #         "raw": text,
# #         "summary": extract_summary(text),
# #         "keywords": extract_keywords(text)
# #     }

# #     metas = []
# #     vecs = []

# #     # embedding t·ª´ng t·∫ßng
# #     for rep_type, rep_text in representations.items():
# #         doc_id = str(uuid.uuid4())
# #         vec = embedder.encode(rep_text, normalize_embeddings=True)

# #         meta = {
# #             "id": doc_id,
# #             "source": os.path.basename(path),
# #             "rep_type": rep_type,   # lo·∫°i bi·ªÉu di·ªÖn (raw / summary / keywords)
# #             "text": rep_text
# #         }

# #         metas.append(meta)
# #         vecs.append(vec)

# #     # Th√™m v√†o FAISS v√† l∆∞u metadata
# #     vecs_np = np.vstack(vecs).astype("float32")
# #     index.add(vecs_np)
# #     docs.extend(metas)

# #     faiss.write_index(index, INDEX_FILE)
# #     with open(META_FILE, "w", encoding="utf-8") as f:
# #         json.dump(docs, f, ensure_ascii=False, indent=2)

# #     print(f"[*] Ingested {path}: {len(metas)} representations")

# # # ---------------------------------------------------------------
# # # üöÄ Ingest to√†n b·ªô file trong th∆∞ m·ª•c ch·ªâ ƒë·ªãnh
# # def ingest_folder(folder=DATA_DIR):
# #     for fname in os.listdir(folder):
# #         p = os.path.join(folder, fname)
# #         if os.path.isfile(p):
# #             ingest_file(p)

# # # ---------------------------------------------------------------
# # if __name__ == "__main__":
# #     print("üöÄ Multi-representation RAG Embedding Started...\n")
# #     ingest_folder(DATA_DIR)
# #     print("\n‚úÖ Ho√†n t·∫•t embedding t·∫•t c·∫£ file.")

# # ingest.py (phi√™n b·∫£n multi-representation RAG - c√≥ thanh ti·∫øn tr√¨nh)
# # ---------------------------------------------------------------
# # M·ª•c ti√™u:
# #   - Duy·ªát qua t·∫•t c·∫£ file .docx (ho·∫∑c vƒÉn b·∫£n) trong th∆∞ m·ª•c ch·ªâ ƒë·ªãnh
# #   - M·ªói file ƒë∆∞·ª£c embedding nhi·ªÅu t·∫ßng bi·ªÉu di·ªÖn (multi-representation)
# #   - Hi·ªÉn th·ªã ti·∫øn ƒë·ªô ingest b·∫±ng thanh ti·∫øn tr√¨nh tqdm
# import sys, io
# sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# import os, json, uuid
# import numpy as np
# import faiss
# from tqdm import tqdm
# from sentence_transformers import SentenceTransformer
# from extractors import auto_extract
# from utils import extract_summary, extract_keywords

# # ---------------------------------------------------------------
# # üîß C·∫•u h√¨nh
# DATA_DIR = "data_output"
# INDEX_FILE = "faiss.index"
# META_FILE = "docs.json"

# MODEL_NAME = "intfloat/multilingual-e5-small"
# embedder = SentenceTransformer(MODEL_NAME)
# dimension = embedder.get_sentence_embedding_dimension()

# # ---------------------------------------------------------------
# # üß† Load ho·∫∑c t·∫°o FAISS index
# if os.path.exists(INDEX_FILE):
#     index = faiss.read_index(INDEX_FILE)
#     with open(META_FILE, "r", encoding="utf-8") as f:
#         docs = json.load(f)
# else:
#     index = faiss.IndexFlatIP(dimension)
#     docs = []

# # ---------------------------------------------------------------
# def ingest_file(path):
#     text = auto_extract(path)
#     if not text.strip():
#         print(f"[!] B·ªè qua file r·ªóng: {path}")
#         return

#     representations = {
#         "raw": text,
#         "summary": extract_summary(text),
#         "keywords": extract_keywords(text)
#     }

#     metas = []
#     vecs = []

#     for rep_type, rep_text in representations.items():
#         doc_id = str(uuid.uuid4())
#         vec = embedder.encode(rep_text, normalize_embeddings=True)

#         meta = {
#             "id": doc_id,
#             "source": os.path.basename(path),
#             "rep_type": rep_type,
#             "text": rep_text
#         }
#         metas.append(meta)
#         vecs.append(vec)

#     vecs_np = np.vstack(vecs).astype("float32")
#     index.add(vecs_np)
#     docs.extend(metas)

#     faiss.write_index(index, INDEX_FILE)
#     with open(META_FILE, "w", encoding="utf-8") as f:
#         json.dump(docs, f, ensure_ascii=False, indent=2)

# # ---------------------------------------------------------------
# def ingest_folder(folder=DATA_DIR):
#     files = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]
#     if not files:
#         print("‚ö†Ô∏è Kh√¥ng c√≥ file n√†o trong th∆∞ m·ª•c c·∫ßn ingest.")
#         return

#     print(f"üìÅ ƒêang ingest {len(files)} file trong th∆∞ m·ª•c: {folder}\n")

#     for path in tqdm(files, desc="üîÑ ƒêang x·ª≠ l√Ω", unit="file", ncols=90):
#         ingest_file(path)

# # ---------------------------------------------------------------
# if __name__ == "__main__":
#     print("üöÄ Multi-representation RAG Embedding Started...\n")
#     ingest_folder(DATA_DIR)
#     print("\n‚úÖ Ho√†n t·∫•t embedding t·∫•t c·∫£ file.")





# ingest.py (phi√™n b·∫£n 2-vector: summary + keywords, chung 1 FAISS, l∆∞u raw text)
# ---------------------------------------------------------------
# M·ª•c ti√™u:
#   - Duy·ªát qua t·∫•t c·∫£ file .docx / .txt / .pdf trong th∆∞ m·ª•c DATA_DIR
#   - M·ªói file sinh 2 vector embedding:
#       + summary: t√≥m t·∫Øt n·ªôi dung
#       + keywords: tr√≠ch xu·∫•t c√°c ch·ªß t·ª´ ch√≠nh
#   - C·∫£ hai vector c√πng n·∫±m trong 1 FAISS index
#   - Metadata (docs.json) l∆∞u raw text ƒë·∫ßy ƒë·ªß
#   - Hi·ªÉn th·ªã ti·∫øn tr√¨nh ingest b·∫±ng tqdm
import sys, io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")

import os, json, uuid
import numpy as np
import faiss
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from src.extractors import auto_extract
from src.utils import extract_summary, extract_keywords

# ---------------------------------------------------------------
# üîß C·∫•u h√¨nh
DATA_DIR = "data_output"
INDEX_FILE = "faiss.index"
META_FILE = "docs.json"

MODEL_NAME = "intfloat/multilingual-e5-small"
embedder = SentenceTransformer(MODEL_NAME)
dimension = embedder.get_sentence_embedding_dimension()

# ---------------------------------------------------------------
# üß† T·∫°o ho·∫∑c load FAISS index
if os.path.exists(INDEX_FILE):
    index = faiss.read_index(INDEX_FILE)
    with open(META_FILE, "r", encoding="utf-8") as f:
        docs = json.load(f)
else:
    index = faiss.IndexFlatIP(dimension)  # cosine similarity
    docs = []

# ---------------------------------------------------------------
# üìÑ H√†m x·ª≠ l√Ω 1 file ƒë∆°n
def ingest_file(path):
    raw_text = auto_extract(path)
    if not raw_text.strip():
        print(f"[!] B·ªè qua file r·ªóng: {path}")
        return

    # T·∫°o 2 bi·ªÉu di·ªÖn: summary + keywords
    summary_text = extract_summary(raw_text)
    keyword_text = extract_keywords(raw_text)

    representations = {
        "summary": summary_text,
        "keywords": keyword_text
    }

    metas, vecs = [], []

    # Duy·ªát qua 2 bi·ªÉu di·ªÖn
    for rep_type, rep_text in representations.items():
        doc_id = str(uuid.uuid4())
        vec = embedder.encode(rep_text, normalize_embeddings=True)

        meta = {
            "id": doc_id,
            "source": os.path.basename(path),
            "rep_type": rep_type,   # summary / keywords
            "text": raw_text        # lu√¥n l∆∞u raw text
        }

        metas.append(meta)
        vecs.append(vec)

    # Th√™m 2 vector v√†o FAISS
    vecs_np = np.vstack(vecs).astype("float32")
    index.add(vecs_np)
    docs.extend(metas)

    # L∆∞u l·∫°i
    faiss.write_index(index, INDEX_FILE)
    with open(META_FILE, "w", encoding="utf-8") as f:
        json.dump(docs, f, ensure_ascii=False, indent=2)

# ---------------------------------------------------------------
# üöÄ Ingest to√†n b·ªô th∆∞ m·ª•c
def ingest_folder(folder=DATA_DIR):
    files = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]
    if not files:
        print("‚ö†Ô∏è Kh√¥ng c√≥ file n√†o trong th∆∞ m·ª•c c·∫ßn ingest.")
        return

    print(f"üìÅ ƒêang ingest {len(files)} file trong th∆∞ m·ª•c: {folder}\n")

    for path in tqdm(files, desc="üîÑ ƒêang x·ª≠ l√Ω", unit="file", ncols=90):
        ingest_file(path)

# ---------------------------------------------------------------
if __name__ == "__main__":
    print("üöÄ Multi-representation (summary + keywords) Embedding Started...\n")
    ingest_folder(DATA_DIR)
    print("\n‚úÖ Ho√†n t·∫•t embedding t·∫•t c·∫£ file.")
